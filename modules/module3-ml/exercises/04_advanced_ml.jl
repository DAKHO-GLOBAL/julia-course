# Exercice 4 : Machine Learning Avanc√© avec Julia
# Module 3 : Apprentissage Automatique avec Julia
# Dur√©e : 75 minutes

# üìö AVANT DE COMMENCER
# Lisez le r√©sum√© d'apprentissage : resume_04_advanced_ml.md
# D√©couvrez les techniques ML de pointe avec applications burkinab√® !

println("üìö Consultez le r√©sum√© : modules/module3-ml/resume_04_advanced_ml.md")
println("Appuyez sur Entr√©e quand vous √™tes pr√™t pour le ML expert...")
readline()

println("üß† Machine Learning Avanc√© : Applications Burkina Faso")
println("="^70)

# TODO 1 : Configuration environnement ML expert (10 minutes)
# Configurez un environnement ML de production avec tous les paquets n√©cessaires

# TODO : Importez MLJ, MLJModels, MLJTuning
using MLJ

# TODO : Importez DataFrames, CSV, Statistics, Random


# TODO : Importez Plots, StatsPlots pour visualisation


# TODO : Importez Dates, LinearAlgebra


# TODO : Importez Clustering, MultivariateStats


# Configuration pour reproductibilit√©
Random.seed!(42)
MLJ.color_off()

println("ü§ñ Environnement ML avanc√© configur√©")

# TODO 2 : G√©n√©ration de donn√©es agricoles r√©alistes (15 minutes)
println("\nüåæ G√©n√©ration de donn√©es agricoles du Burkina Faso")

# TODO : Cr√©ez une fonction pour g√©n√©rer des donn√©es agricoles r√©alistes
# Cette fonction doit prendre en compte :
# - Les 13 r√©gions du Burkina Faso
# - Les types de sol (Ferrugineux, Vertisol, Sols-bruns, Lithosol, Hydromorphe)  
# - Les cultures principales (Mil, Sorgho, Ma√Øs, Riz, Coton, Ni√©b√©, Arachide)
# - Les facteurs climatiques (temp√©rature, pr√©cipitations)
# - Les pratiques agricoles (engrais, irrigation, m√©canisation)

function g√©n√©rer_donn√©es_agricoles_bf(n_observations=1000)
    println("G√©n√©ration de donn√©es agricoles burkinab√®...")

    # TODO : D√©finissez les r√©gions du BF
    r√©gions = [
    # TODO : Listez les 13 r√©gions
    ]

    # TODO : D√©finissez les types de sol
    sols = [
    # TODO : Types de sol burkinab√®
    ]

    # TODO : D√©finissez les cultures principales  
    cultures = [
    # TODO : Cultures burkinab√®
    ]

    donn√©es = DataFrame()

    for i in 1:n_observations
        # TODO : S√©lectionnez al√©atoirement r√©gion, culture, sol
        r√©gion = rand(r√©gions)
        # TODO : Compl√©tez...

        # TODO : G√©n√©rez des param√®tres climatiques r√©alistes selon la r√©gion
        # Astuce : Sahel = moins de pluie, Centre = interm√©diaire, Sud = plus de pluie
        if r√©gion in ["Sahel", "Nord"]
            pr√©cipitations = 11111
            # TODO : 300-700mm pour zone sah√©lienne
            temp√©rature_moy = 11111
            # TODO : 28-36¬∞C
        elseif r√©gion in ["Centre", "Plateau-Central"]
            # TODO : Zone soudano-sah√©lienne
        else  # Sud
            # TODO : Zone soudanienne
        end

        # TODO : G√©n√©rez variables agricoles
        superficie = # TODO : 0.5-5 hectares (petits producteurs)
            engrais_kg_ha = # TODO : 0-100 kg/ha (30% des producteurs utilisent)
                irrigation = # TODO : 5% ont acc√®s √† l'irrigation

                # TODO : Calculez le rendement bas√© sur un mod√®le r√©aliste
                # Facteurs : climat, sol, pratiques agricoles, r√©gion
                    rendement = # TODO : Utilisez les facteurs pour calculer rendement r√©aliste

                    # TODO : Ajoutez l'observation au DataFrame
                        push!(donn√©es, (
                            r√©gion=r√©gion,
                            culture=culture,
                            type_sol=sol,
                            # TODO : Ajoutez toutes les variables
                        ))
    end

    return donn√©es
end

# TODO : G√©n√©rez le dataset avec 1500 observations
df_agri = # TODO
    println("‚úÖ Dataset g√©n√©r√© : $(nrow(df_agri)) observations")

# TODO 3 : Pr√©paration des donn√©es pour ML (10 minutes)
println("\nüîß Pr√©paration des donn√©es pour Machine Learning")

# TODO : Convertissez les variables cat√©gorielles avec categorical()
# df_ml = copy(df_agri)
# df_ml.r√©gion_encoded = 
# df_ml.culture_encoded = 
# df_ml.sol_encoded = 

# TODO : S√©lectionnez les features pour le mod√®le
features = [
# TODO : Listez vos features num√©riques et cat√©gorielles
]

# TODO : Cr√©ez X et y
X = # TODO : select(df_ml, features)
    y = # TODO : df_ml.rendement

    # TODO : Division train/test avec partition()
    # Utilisez 80% pour l'entra√Ænement
        train_idx, test_idx = # TODO
            X_train, X_test = # TODO
                y_train, y_test = # TODO
                    println("Donn√©es d'entra√Ænement : $(length(train_idx)) observations")
println("Donn√©es de test : $(length(test_idx)) observations")

# TODO 4 : Mod√®le 1 - Random Forest (15 minutes)
println("\nüå≤ Mod√®le 1 : Random Forest Ensemble")

# TODO : Chargez RandomForestRegressor
RandomForestRegressor = @load RandomForestRegressor pkg = DecisionTree

# TODO : Cr√©ez et configurez le mod√®le Random Forest
rf_model = # TODO : Configurez avec n_trees=100, max_depth=10

# TODO : Cr√©ez la machine MLJ
    rf_machine = # TODO

    # TODO : Entra√Ænez le mod√®le
        println("Entra√Ænement Random Forest...")
# TODO : fit!(rf_machine)

# TODO : Faites des pr√©dictions
rf_predictions = # TODO : predict(rf_machine, X_test)

# TODO : Calculez les m√©triques de performance
    rf_mae = # TODO : mean(abs.(rf_predictions - y_test))
        rf_rmse = # TODO : sqrt(mean((rf_predictions - y_test).^2))
            rf_r2 = # TODO : 1 - sum((y_test - rf_predictions).^2) / sum((y_test .- mean(y_test)).^2)
                println("Performance Random Forest :")
println("  MAE : $(round(rf_mae, digits=3)) t/ha")
println("  RMSE : $(round(rf_rmse, digits=3)) t/ha")
println("  R¬≤ : $(round(rf_r2, digits=3))")

# TODO 5 : Mod√®le 2 - Gradient Boosting (10 minutes)
println("\nüöÄ Mod√®le 2 : Gradient Boosting")

# TODO : Chargez EvoTreeRegressor (plus stable que XGBoost)
try
    EvoTreeRegressor = @load EvoTreeRegressor pkg = EvoTrees
    # TODO : Cr√©ez et entra√Ænez le mod√®le
    gb_model = # TODO
        gb_machine = # TODO
            println("Entra√Ænement Gradient Boosting...")
    # TODO : fit!(gb_machine)

    # TODO : Pr√©dictions et √©valuation
    gb_predictions = # TODO
        gb_mae = # TODO
            gb_r2 = # TODO
                println("Performance Gradient Boosting :")
    println("  MAE : $(round(gb_mae, digits=3)) t/ha")
    println("  R¬≤ : $(round(gb_r2, digits=3))")

catch e
    println("‚ö†Ô∏è EvoTrees non disponible : $e")
    println("Continuons avec les autres mod√®les...")
end

# TODO 6 : Clustering des r√©gions climatiques (10 minutes)
println("\nüó∫Ô∏è Clustering des R√©gions Climatiques du BF")

# TODO : Pr√©parez les donn√©es pour clustering
println("Analyse climatique des r√©gions burkinab√®...")

# TODO : Calculez les statistiques par r√©gion
donn√©es_r√©gions = combine(groupby(df_agri, :r√©gion)) do group
    DataFrame(
    # TODO : Calculez moyennes des variables climatiques par r√©gion
    # pr√©cipitations_moy = 
    # temp√©rature_moy = 
    # rendement_moy = 
    )
end

# TODO : Standardisez les donn√©es pour clustering
Standardizer = @load Standardizer
standardizer = Standardizer()

# TODO : S√©lectionnez variables pour clustering
vars_clustering = [
# TODO : Variables climatiques importantes
]

X_cluster = # TODO : select(donn√©es_r√©gions, vars_clustering)

# TODO : Standardisez les donn√©es
    std_machine = # TODO
    # TODO : fit! et transform

    # TODO : Appliquez K-means clustering
        using Clustering

# TODO : Convertissez en matrice pour Clustering.jl
X_matrix = # TODO

# TODO : Testez diff√©rents nombres de clusters (2 √† 6)
    println("Recherche du nombre optimal de clusters...")
for k in 2:6
    # TODO : Appliquez k-means
    kmeans_result = # TODO
        println("k=$k : co√ªt total=$(kmeans_result.totalcost)")
end

# TODO : Choisissez k optimal et faites clustering final
k_optimal = 3  # Ajustez selon vos r√©sultats
kmeans_final = # TODO
    clusters = assignments(kmeans_final)

# TODO : Ajoutez clusters au DataFrame
donn√©es_r√©gions.cluster = clusters

println("\nüîç R√©sultats du clustering :")
for i in 1:k_optimal
    r√©gions_cluster = donn√©es_r√©gions[donn√©es_r√©gions.cluster.==i, :r√©gion]
    println("  Cluster $i : $(join(r√©gions_cluster, ", "))")
end

# TODO 7 : Visualisations des r√©sultats (10 minutes)
println("\nüìä Visualisation des r√©sultats ML")

# TODO : Graphique 1 - Pr√©dictions vs R√©alit√©
p1 = scatter(y_test, rf_predictions,
    title="Pr√©dictions vs R√©alit√© - Random Forest",
    xlabel="Rendement R√©el (t/ha)",
    ylabel="Rendement Pr√©dit (t/ha)",
    alpha=0.6, size=(600, 500))

# TODO : Ajoutez la ligne y=x pour pr√©diction parfaite
plot!([minimum(y_test), maximum(y_test)],
    [minimum(y_test), maximum(y_test)],
    color=:red, linestyle=:dash, linewidth=2, label="Pr√©diction Parfaite")

display(p1)

# TODO : Graphique 2 - Clustering des r√©gions
# TODO : Cr√©ez un scatter plot avec les coordonn√©es des r√©gions
# Colorez selon les clusters trouv√©s

# Coordonn√©es approximatives des r√©gions (fournies)
coords_r√©gions = DataFrame(
    r√©gion=["Sahel", "Nord", "Centre-Nord", "Centre", "Plateau-Central",
        "Est", "Centre-Est", "Boucle du Mouhoun", "Hauts-Bassins",
        "Centre-Ouest", "Sud-Ouest", "Cascades", "Centre-Sud"],
    latitude=[14.0, 13.5, 13.2, 12.4, 12.3, 12.0, 11.9, 12.3,
        11.2, 12.1, 10.3, 10.8, 11.2],
    longitude=[-0.2, -2.3, -1.5, -1.5, -1.2, 0.5, -0.3, -2.9,
        -4.3, -2.3, -3.2, -4.3, -1.0]
)

# TODO : Fusionnez donn√©es r√©gions avec coordonn√©es
r√©gions_avec_coords = # TODO : leftjoin(donn√©es_r√©gions, coords_r√©gions, on=:r√©gion)

# TODO : Cr√©ez le graphique de clustering g√©ographique
    p2 = # TODO : scatter plot avec longitude/latitude color√© par cluster
        display(p2)

println("‚úÖ Visualisations cr√©√©es !")

# TODO 8 : D√©fi bonus - Analyse de s√©ries temporelles (Optionnel)
println("\nüìà D√©fi Bonus : Analyse Temporelle")

# TODO : Si vous avez le temps, cr√©ez des donn√©es temporelles
# et analysez l'√©volution des rendements sur plusieurs ann√©es

# TODO 9 : √âvaluation et comparaison de mod√®les
println("\nüèÜ Comparaison finale des mod√®les")

# TODO : Cr√©ez un tableau comparatif des performances
# Comparez MAE, RMSE, R¬≤ des diff√©rents mod√®les

println("ü•á Meilleur mod√®le bas√© sur R¬≤ : Random Forest")

# Bilan d'apprentissage
println("\nüìà BILAN D'APPRENTISSAGE")
println("="^70)
println("üß† F√âLICITATIONS ! MA√éTRISE DU ML AVANC√â AVEC JULIA !")
println("="^70)
println("‚úÖ Techniques ML avanc√©es ma√Ætris√©es :")
println("  üå≤ Random Forest pour pr√©diction de rendements")
println("  üöÄ Gradient Boosting avec EvoTrees.jl")
println("  üó∫Ô∏è Clustering K-means pour segmentation r√©gionale")
println("  üìä √âvaluation rigoureuse avec m√©triques multiples")
println("  üìà Visualisation des r√©sultats ML")
println("  üáßüá´ Applications contextualis√©es agriculture BF")

println("\nüåü BADGE D√âBLOQU√â : 'Expert ML Burkina Faso'")
println("Vous ma√Ætrisez maintenant les techniques avanc√©es pour")
println("r√©soudre des probl√®mes agricoles complexes !")

println("\nüöÄ PROCHAINE √âTAPE : 05_python_bridge.jl")
println("   (Combinez Julia et Python pour maximum de puissance !)")

# NOTES POUR L'INSTRUCTEUR :
# Cet exercice couvre :
# 1. G√©n√©ration de donn√©es r√©alistes contextualis√©es
# 2. Preprocessing et feature engineering
# 3. Mod√®les ensemble (Random Forest, Gradient Boosting)
# 4. Clustering non-supervis√© 
# 5. √âvaluation et validation rigoureuse
# 6. Visualisation des r√©sultats
# 7. Applications agriculture sah√©lienne

# Extensions possibles :
# - Hyperparameter tuning avec MLJTuning
# - Cross-validation avec MLJ
# - Feature importance analysis
# - Time series forecasting
# - Deep learning avec Flux.jl